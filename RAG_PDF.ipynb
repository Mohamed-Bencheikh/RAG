{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-Bencheikh/RAG/blob/main/RAG_PDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***RAG: Chat with your documents***"
      ],
      "metadata": {
        "id": "3cuUv0CGWsTh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGoKG3E4k6l"
      },
      "source": [
        "# **DEPENDENCIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCID9WpnFmUB"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfjcqb35FdRu"
      },
      "outputs": [],
      "source": [
        "# !pip install unstructured[all-docs] unstructured\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSCE2ZZKswBU"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aup3pm81NmYV"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Hg_kDm6deF"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XJbuLJh_jCH"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hFa36ugiPu__"
      },
      "outputs": [],
      "source": [
        "# !cat /usr/local/bin/ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-3uyIoh6wxS"
      },
      "source": [
        "# **IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gBodC8AzEGy4"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama"
      ],
      "metadata": {
        "id": "QP1NVMmno2Wv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pIPxsFd7HDMA"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UgnEOkbj7CI2"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ofh-jZsm7b36"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import asyncio\n",
        "from aiohttp import ClientSession\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "import queue\n",
        "from threading import Thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-KJmp7ER8oMg"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14veKVeVHto5"
      },
      "source": [
        "# **LOADING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0MAH8S7EtuY"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/LLMs enhanced CF.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiU2_Mc1FORT",
        "outputId": "7b2395d4-8d03-472b-c126-25b2672713cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "loader = UnstructuredPDFLoader(file_path)\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "6d79JGh2Fuq6",
        "outputId": "dd5b75b2-1400-4898-959a-b57c4e8bac18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4 2 0 2\\n\\nr a\\n\\nM 6 2\\n\\n]\\n\\nR\\n\\nI . s c [\\n\\n1 v 8 8 6 7 1 . 3 0 4 2 : v i X r a\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nZhongxiang Sun Gaoling School of Artificial Intelligence Renmin University of China Beijing, China sunzhongxiang@ruc.edu.cn\\n\\nZihua Si Gaoling School of Artificial Intelligence Renmin University of China Beijing, China zihua_si@ruc.edu.cn\\n\\nXiaoxue Zang Kuaishou Technology Co., Ltd. Beijing, China zangxiaoxue@kuaishou.com\\n\\nKai Zheng Kuaishou Technology Co., Ltd. Beijing, China zhengkai@kuaishou.com\\n\\nYang Song Kuaishou Technology Co., Ltd. Beijing, China yangsong@kuaishou.com\\n\\nXiao Zhang Jun Xu Gaoling School of Artificial Intelligence Renmin University of China Beijing, China {zhangx89,junxu}@ruc.edu.cn\\n\\nABSTRACT Recent advancements in Large Language Models (LLMs) have at- tracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs). Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs. Al- though the extensive world knowledge embedded in LLMs gener- ally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collab- orative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs. In this pa- per, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which dis- tils the world knowledge and reasoning capabilities of LLMs into collaborative filtering. We also explored a concise and efficient instruction-tuning method, which improves the recommendation capabilities of LLMs while preserving their general functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive ex- periments on three real-world datasets demonstrate that LLM-CF significantly enhances several backbone recommendation models and consistently outperforms competitive baselines, showcasing its effectiveness in distilling the world knowledge and reasoning capabilities of LLM into collaborative filtering.\\n\\nCCS CONCEPTS • Information systems → Recommender systems.\\n\\nKEYWORDS Recommender System, Large Language Model, Collaborative Filter- ing\\n\\nACM Reference Format: Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu. 2018. Large Language Models Enhanced Collaborative Filtering. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX\\n\\n1 INTRODUCTION Large Language Models (LLMs) [29, 39] have made rapid advance- ments, showcasing remarkable capabilities [42] in context compre- hension, reasoning, generalization, and modeling world knowledge, and so on. With the advancement of Large Language Models (LLMs), many researchers are focusing on how to utilize LLMs in recom- mendation systems (RSs). Many studies have already applied LLMs to various aspects of RSs, including ranking [50], Click-Through Rate (CTR) prediction [2, 8, 48], sequential recommendation [13], rating prediction [21], and data augmentation [28, 43]. Consider- ing specific methods to utilize LLMs for RSs, current applications can be classified into two categories. (1) LLMs as RSs: LLMs can be directly prompted or be fine-tuned to function as specialized RSs [2, 8, 36, 48, 50]. (2) LLM-enhanced RSs: Based on world knowledge and reasoning abilities, LLM-derived embedding vec- tors and LLM-generated texts can enhance RSs [13, 46, 48, 51].\\n\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, June 03–05, 2018, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX\\n\\nDespite their effectiveness, there are still several challenges to be addressed. LLMs as RSs suffers from low efficiency due to the resource-intensive nature of LLMs, making their practical ap- plication challenging. LLM-enhanced RSs inadequately exploit collaborative filtering information because the LLM can only take a limited number of users and items as inputs. How to better leverage LLMs to provide enhanced collaborative filtering information to existing RSs becomes key in LLM-enhanced RSs.\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nRecGen-LLaMA\\n\\nReasoning andWorld Knowledge\\n\\nCoTRecommendation Features\\n\\nOnline serviceOffline serviceSimilar ExamplesIn-context CoT Dataset\\n\\nRecommendation ModelRetrieveWorld Knowledge and Reasoning guided CF LabelRecommendation Data\\n\\nInstructionTuning+\\n\\nGeneral Data\\n\\nUserItem\\n\\nFigure 1: LLM-CF integrates LLM-based world knowledge and reasoning with collaborative filtering to improve rec- ommendation performance, using LLMs with recommender capability and decoupled latency-free offline generation.\\n\\nConsidering the challenges in deploying LLMs as RSs due to their inherently extensive parameterization, we focus on LLM- enhanced RSs, which are more applicable and flexible for ex- isting RSs. In order to better guide collaborative filtering to en- hance existing RSs with LLMs. Inspired by Chain-of-Thought (CoT) and In-Context Learning [4, 10] in LLMs, we propose a novel Large Language Models enhanced Collaborative Filtering (LLM- CF) Framework, which distils the world knowledge and reasoning capabilities of LLM into collaborative filtering in an in-context, chain of thought methodology. As shown in Figure 1, LLM-CF can be decoupled into two parts: (1) offline service part (§ 4): Fine- tune LLM to enhance its recommendation capabilities, generate CoT reasoning with collaborative filtering information, and construct in-context CoT dataset. (2) online service part (§ 5): Retrieve the in-context CoT examples, learn the world knowledge and reasoning guided Collaborative Filtering (CF) feature, and use this feature to enhance existing RSs.\\n\\nIn the offline service, we perform instruction tuning on LLM to obtain CF information about users and items in the recommenda- tion data. However, our initial findings indicate that full parameter tuning LLMs could result in substantial forgetting of their gen- eral capabilities, as discussed in § 4.2. We leveraged a simple but effective data mixing method to finetune LLaMA2 [39], and success- fully trained a model RecGen-LLaMA, which achieves an optimal balance between general and recommendation capabilities. Then, we use RecGen-LLaMA to generate CoT reasoning for a subset of examples in training data, forming the in-context CoT dataset.\\n\\nIn the online service, the retrieval module uses a query composed of the textual features of the current recommendation features to perform embedding-based retrieval on the in-context CoT dataset, forming in-context CoT examples. These retrieved examples con- tain similar recommendation features, as well as CoT reasoning generated by RecGen-LLaMA. The in-context CoT examples are concatenated with the current recommendation features and then fed into the In-context Chain of Thought (ICT) module of LLM-CF to learn world-knowledge and reasoning guided CF feature. Finally, the enhanced CF feature is fed into the backbone recommendation model for making the final prediction.\\n\\nAdvantage: LLM-CF not only leverages LLMs to provide en- hanced collaborative filtering information to existing RSs but also\\n\\nZhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu\\n\\nachieves exceptional deployment efficiency. (1) We teach collab- orative filtering knowledge from recommendation data to LLMs, ensuring that the generated texts include a comprehensive under- standing of user behaviors and item features. (2) When integrating the world knowledge and reasoning capabilities from LLMs into RSs, We design ICT modules that embody both these capabilities and collaborative filtering information. This information comes from the explicit collaborative filtering information contained in the retrieved similar user histories and the RecGen-LLaMA gener- ated CoT reasoning. (3) The time cost of LLM-CF is manageable, as we only require the offline serving of LLM without the need for online inference alongside the RSs.\\n\\nWe summarize the major contributions of this paper as follows: (1) We represent pioneering work in using the LLMs-based world knowledge and reasoning to guide collaborative filtering features, thereby enhancing conventional recommendation models.\\n\\n(2) The proposed LLM-CF is inspired by in-context learning and chain of thought reasoning in LLMs, which effectively distills the world knowledge and reasoning capabilities of LLMs into conven- tional recommendation models in an in-context chain of thought manner. Moreover, the LLM-CF is more efficient compared to ex- isting LLM-enhanced RSs by decoupling LLM generation from the recommendation system’s online services.\\n\\n(3) We conducted extensive experiments on three public datasets. The experimental results demonstrate that the LLM-CF could signif- icantly improve the recommendation performance of conventional recommendation models in both ranking and retrieval tasks, veri- fying the effectiveness of the LLM-CF.\\n\\n2 RELATED WORK 2.1 LLM as RSs LLMs as RSs involve directly prompting LLMs to make recommen- dations using natural language-based queries or adapting LLMs to serve as RSs after fine-tuning them with recommendation data. P5 [11] transforms user interaction data into text prompts using item indices for training language models. In contrast, TALLRec [2] utilizes instructional designs to outline recommendation tasks and adapts LLMs through fine-tuning to follow these guidelines, thereby producing recommendations. Further, ReLLa [24] uses retrieved user history to fine-tune LLMs, addressing the issue of LLMs’ weak capability in processing long user sequences. LLaMARec [50] ini- tially applies small-scale recommenders to select candidates from user interaction history. Then, this history and the chosen items are fed into the LLM as text using a specially crafted prompt template. However, directly using LLMs as inference models for recommen- dation tasks presents challenges, such as high computational costs and slow inference times, causing challenges in meeting the re- quirements for online services and deployment.\\n\\n2.2 LLM-enhanced RSs LLM-enhanced RSs leverage the world knowledge and reasoning abilities of LLMs by utilizing them to generate knowledge-rich texts or employ LLM-derived embeddings as features to enhance RSs. GIRL [51] applies an LLM, fine-tuned with job datasets, to create job descriptions from CVs, boosting traditional job RSs. KAR [46]\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nUser FeaturesItem Features\\n\\nGender: FemaleAge: 25Interacted Items:[title: OPI Red Shatter Crackle Nail Polish E55 New; brand: OPI; category: Beauty, Makeup, Nails, Nail Polish] Title: Suave Professionals Moroccan Infusion Shine Shampoo, 12.6 Ounce; Brand: Suave; Category: Beauty, Hair Care, ShampoosInstruction: Based on the following purchase history of a user, please determine whether the user is likely to purchase the target new item. Answer with “Yes” or “No”.Input: The User:{User Features}.Target new item:{Item Features}. Output: No.Instruction-Tuning Prompt\\n\\nFigure 2: Example of the textual format recommendation features and Instruction-Tuning prompt.\\n\\nuses LLMs for generating user Preference Reasoning and item Fac- tual Knowledge, enhancing RSs through hybrid-expert adaptors. ONCE [25] explores both open and closed-source LLMs for RS enhancement; open-source LLMs as feature encoders and closed- source via prompt learning. HKF [48] employs LLMs to merge diverse user behavior data, improving RSs by integrating these semantic features. However, these models primarily focus on lim- ited user-item information, neglecting collaborative filtering in- formation, and suffer from efficiency issues due to real-time LLM processing for new interactions or items. The proposed LLM-CF addresses these by distilling LLMs’ knowledge and reasoning with collaborative filtering into existing RSs, separating LLM generation from online services to achieve efficient LLM-enhanced RSs.\\n\\n3 PRELIMINARIES In this section, we define the recommendation task as a binary classification problem using multi-field categorical data. The dataset D = {(x𝑛, 𝑦𝑛)}𝑁 𝑛=1 comprises pairs of recommendation features x𝑖 and binary labels 𝑦𝑖 . Recommendation features include user features (e.g., User ID, gender, age) and item features (e.g., item ID, brand). The label 𝑦𝑖 indicates a click (1) or no click (0). The goal is to learn a function 𝑓 (·) with parameters 𝜃 to predict click probabilities, ˆ𝑦𝑖 = 𝑓 (x𝑖 ; 𝜃 ), for each x𝑖 .\\n\\nTo meet the requirements of large language models, we follow the instruction prompt used in [2, 24], as shown in Figure 2, which involves extracting textual format recommendation features x𝑡 𝑖 from recommendation features x𝑖 and organizing it into a \"Task Instruction\". This instruction guides the LLM to determine whether a user is likely to be interested in the target item based on the user’s historical interactions and the other user features. The LLM generates a binary response of \"Yes\" or \"No\", with \"Yes\" indicat- ing a positive interaction (click) and \"No\" indicating a negative interaction (no click).\\n\\nTo study the world knowledge and reasoning guided collabora- tive filtering feature based on LLMs, we propose LLM-CF (Large Language Models enhanced Collaborative Filtering). The overview of LLM-CF is illustrated in Figure 3. The proposed LLM-CF can be decoupled into the offline service part (§ 4) and the online service part (§ 5).\\n\\n4 OFFLINE SERVICE OF LLM-CF In this section, we introduce the offline service of LLM-CF in detail.\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\n4.1 Overview The offline service part of LLM-CF includes the following process: Training of RecGen-LLaMA: LLMs equipped with recommen- dation capability can better comprehend the collaborative filtering information within recommendation data, thereby generating im- proved textual to enhance conventional recommendation models. However, our experiments reveal that directly following the previ- ous work [2] by using recommendation data to fine-tune LLMs leads to catastrophic forgetting of general capabilities. This results in a significant decline in the LLM benchmark to the extent that cannot generate meaningful text. To solve this challenge, we conducted ex- tensive experiments and found a concise and efficient full-parameter instruction-tuning method. This method, which integrates recom- mendation data with general instruction-tuning data, optimizes the balance between the model’s general and recommendation abilities. By applying this method to the widely-used LLaMA2 [39], we suc- cessfully trained RecGen-LLaMA, achieving an optimal balance between general and recommendation capabilities.\\n\\nCoT Reasoning Generation: For the recommendation data (xi, 𝑦𝑖 ) ∈ D, we designed a zero-shot CoT prompt that decom- poses user-item interaction and then reconstructs them, thereafter inducing RecGen-LLaMA to generate CoT reasoning 𝑐 based on the textual representation of recommendation features x𝑡 . These {𝑐1, . . . , 𝑐𝑚, . . . , 𝑐𝑀 } along with the original recommendation data form the In-Context CoT Dataset C = {(x𝑚, 𝑐𝑚, 𝑦𝑚)}𝑀\\n\\n𝑚=1.\\n\\n4.2 RecGen-LLaMA In this section, we explore how to fine-tune an LLM that balances recommendation capability with general capabilities by conducting extensive experimental analysis.\\n\\nAnalysis Setup: For the empirical study, we selected the Ama- zon [14] Beauty and Sports datasets to evaluate the recommenda- tion capability, with detailed statistical data provided in § 6.1.1. For evaluating the general capability of LLMs, we employed the MMLU benchmark [15], which is widely utilized in evaluating LLM general capabilities [29, 38, 39]. The evaluation metrics include the AUC (Area Under the Curve) for recommendation capability and ACC (Accuracy) for general capability. Regarding the LLM, we chose the currently wildly used LLaMA2-7B-chat [39] as our base model.\\n\\nWe systematically explore diverse methodologies to optimize LLaMA’s recommendation performance while retaining its general capabilities. Our investigation encompasses Continual Learning, Robust Representation Fine-tuning, and Parameter-efficient Fine- tuning approaches: (1) Base: Utilize LLaMA2 directly without any fine-tuning. (2) Half: Fine-tune LLaMA2 directly using half the amount of the recommendation data. (3) Full: Fine-tune LLaMA2 directly using all the recommendation data. (4) LoRA: Given that parameter-efficient fine-tuning methods can preserve most of the model parameters and thus retain the model’s general capabilities, we adopted Low-Rank Adaptation [18] on the Query and Value matrices of LLaMA2, adding parameters with a rank of 8. (5) NeFT: Adding noise during fine-tuning can prevent overfitting or represen- tation collapse of the pre-trained model [19, 49]. We incorporated noise in the embedding layer of the input when fine-tuning LLaMA2 with recommendation data. (6) R3F: Aghajanyan et al. [1] proposed a Fine-tuning method rooted in an approximation to the trust region,\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nZhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu\\n\\nℒ!\"Reconstruction Loss\\n\\nIn-context CoT Examples RetrievalQuery……\\n\\n𝒟: Training Dataset\\n\\nReco Model Feature Encoding\\n\\nTop-K……𝐰!∶\\n\\n𝑥!: Reco Features\\n\\nICT Example ℰK\\n\\nEmbℰ’:𝐜$:\\n\\nTransformer-Decoder𝐫!:\\n\\nMLP\\n\\n\"𝑦!: Label\\n\\n𝐡(𝐫$):ℒ!’:\\n\\nℒ!#\\n\\n𝐫$:𝒍$:\\n\\nICT\\n\\nRecGen-LLaMA\\n\\nReco Model Feature Encoding\\n\\n𝒙!:Reco Features𝑐$: CoT𝑦!:Label\\n\\nReco ModelFeature Interaction\\n\\nPull𝐜$:\\n\\n𝒞: In-Context CoT Dataset\\n\\nCoT ReasoningGeneration<<SYS>> As an AI model developed for analyzing consumer behavior, your task is to generate a chain of thought that considers the following points:1. Utilize the user\\'s interaction history and review comments to summarize their profiles.2. Introduce the target new item and detail its features precisely. In addition, integrate information about items related to the current target new item ...3. Contemplate the alignment between the user\\'s profile and the features of the target item.4. Reflect on the user\\'s potential desire for diversity in their purchases.Your output should be a clear and logical chain of thought... Ensure your analysis is impartial... Focus should be on understanding factors that influence the user‘s decision-making regarding the target item. <</SYS>>Please generate a chain of thought based on the user‘s …considering how these might relate to their interest in the target new item.{Recommendation Features}User\\'s decision-making: The user {Ground-Truth Label} the target new item.Let\\'s think step by step and develop the chain of thought for the above considerations. Commence with the chain of thought immediately:LLM enhanced CF Feature\\n\\nICT Example ℰ\"\\n\\nSample\\n\\nChain of Thought Prompt\\n\\n𝐫!:\\n\\nFigure 3: The overall architecture of the proposed model LLM-CF. (1) CoT Reasoning Generation: Utilize Chain of Thought Prompt to generate CoT reasoning based on RecGen-LLaMA. (2) In-context CoT Examples Retrieval: Identify top-K similar historical recommendation data with current recommendation data to form in-context CoT examples. (3) In-context Chain of Thought (ICT) Module: Employ transformer decoder layers to learn world-knowledge and reasoning guided CF feature.\\n\\nBase\\n\\n75\\n\\nRec(Half)Gen\\n\\n30\\n\\n25\\n\\nLoRA\\n\\n45MMLU ACC (%)\\n\\n80\\n\\nWise-FT(0.3)\\n\\nRecGen\\n\\n70\\n\\nFull\\n\\nR3F\\n\\n35\\n\\n50\\n\\nWise-FT(0.9)\\n\\nNeFT\\n\\nWise-FT(0.6)\\n\\n55\\n\\n85Amazon Beauty AUC (%)\\n\\nHalf\\n\\n40\\n\\nRecGen (Ours)RecGen (Ours)\\n\\nRecGen\\n\\n45MMLU ACC (%)\\n\\nRecGen (Ours)RecGen (Ours)\\n\\n85.0\\n\\nWise-FT(0.3)\\n\\nR3F\\n\\nWise-FT(0.6)\\n\\nNeFT\\n\\n40\\n\\nLoRA\\n\\n87.5Amazon Sports AUC (%)\\n\\n35\\n\\nRec(Half)Gen\\n\\n55\\n\\nWise-FT(0.9)\\n\\n77.5\\n\\n80.0\\n\\n30\\n\\n82.5\\n\\n25\\n\\nFull\\n\\nBase\\n\\n50\\n\\nHalf\\n\\n(7) Wise-FT: Wortsman et al. [44] leveraged the weighted ensem- ble of the pre-trained model and the fine-tuned model to enhance various capabilities of the visual models, with the ensemble weight defined as 𝑊ensemble = 𝛼 ·𝑊fine-tuned + (1 − 𝛼) ·𝑊pre-trained. We en- sembled the original LLaMA2 and LLaMA2 fine-tuned with recom- mendation data, setting 𝛼 to 0.3, 0.6, and 0.9 to obtain Wise-FT(0.3), Wise-FT(0.6), and Wise-FT(0.9) respectively. (8) RecGen: Dong et al. [9] found that the coding and mathematical abilities of LLMs, along with their general capabilities, can be better balanced through a mixed-data fine-tuning approach. We adopt to utilize a mix of recommendation data and high-quality general data (i.e. LIMA [52] and alpaca-gpt4 [30]) for fine-tuning LLaMA2.\\n\\nResults and Findings: We presented the results in Figure 4. As shown in the figure, using the off-the-shelf LLaMA2 model (Base) for recommendation tasks has poor performance. However, fine- tuning LLaMA2 with recommendation data (Full) leads to optimal recommendation performance on both Amazon Sports and Amazon Beauty, but it diminishes LLaMA2’s general capabilities. We also found that RecGen and Wise-FT(0.6) achieve a good balance be- tween general capabilities and recommendation performance, with RecGen showing superior results. Based on RecGen, we devel- oped RecGen-LLaMA to enhance conventional recommendation models.\\n\\nFigure 4: The recommendation and general capabilities of LLaMA2 after fine-tuning on different recommendation data using various methods.\\n\\nwhich ensures that the pre-trained models do not forget the original pre-trained representations when they are fine-tuned for new tasks.\\n\\n4.3 CoT Reasoning Generation To enable RecGen-LLaMA to generate CoT reasoning with world knowledge and collaborative filtering information, we design the Chain of Thought prompt P. As shown in Figure 3, this prompt can decompose user-item interactions and then reconstruct them to\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nTable 1: Comparing LLM-CF with KAR in terms of the effi- ciency of using LLMs to generate data for dynamic scenarios. (cid:34) indicates that it does not require the real-time generation of new data; (cid:37) indicates the opposite.\\n\\nScenario New users New items\\n\\n(cid:37) (cid:37) New interaction (no new items/users) (cid:34)\\n\\nKAR LLM-CF\\n\\n(cid:34) (cid:34) (cid:34)\\n\\nanalyse the relation between them, aiming to analyze user data by simulating the step-by-step reasoning process of humans. Taking product recommendation as an example: Firstly, we had RecGen- LLaMA systematically analyze the user’s interaction history and feedback comments to establish a detailed user profile. Next, RecGen- LLaMA introduced the target new product and its related features in detail to better understand the target product. Finally, RecGen- LLaMA further analyses the alignment between the user profile and the target product features, reflecting on the user’s potential needs for shopping diversity. For the training recommendation data (xi, 𝑦𝑖 ) ∈ D, this generation process can be represented as:\\n\\n𝑐𝑖 = RecGen-LLaMA(xi, 𝑦𝑖, P).\\n\\nConsidering the resource constraints in real recommendation scenarios, we can sample 𝑀 training examples from D for CoT reasoning generation. The sampling process can be random sam- pling or select representative user interaction histories. In this paper, we adopted the uniform random sampling, resulting in {𝑐1, ..., 𝑐𝑚, ..., 𝑐𝑀 } combined with the original recommendation ex- amples to form the In-context CoT dataset C = {(x𝑚, 𝑐𝑚, 𝑦𝑚)}𝑀 𝑚=1.\\n\\n4.4 Efficiency Analysis of Offline Service When deployed, LLM-CF shows high efficiency compared to pre- vious LLM-enhanced RSs work. It avoids real-time generation by LLMs, requiring only periodic updates on the history dataset and successfully decoupling LLM generation from the recommendation system’s online services. Next, we will analyze the time efficiency from the Training and Generation of RecGen-LLaMA.\\n\\nTraining: Based on the experiment results in Figure 4, we dis- covered that training with half the amount of recommendation data (i.e., Half) as compared to using the full dataset (i.e., Full) achieved better general capabilities and nearly the same recom- mendation performance. This phenomenon was also observed in the case of RecGen-LLaMA trained with half the recommendation data (i.e., Rec(Half)Gen). This further proves that in real-world Recommender Systems scenarios, we can use a small amount of rec- ommendation data to fine-tune LLMs to enhance recommendation capability, thereby significantly reducing the training overhead of RecGen-LLaMA in real-world Recommender Systems.\\n\\nGeneration: In the generation phase, we compared LLM-CF with the currently most efficient LLM-enhanced RSs approach KAR [46], which achieves a certain speed-up with prestore genera- tion when user features and item features are relatively fixed. In our efficiency analysis, we have more thoroughly considered three scenarios (i.e., new interactions, new users, new items). Our model’s decoupling of generation and online recommendation effectiveness\\n\\n(1)\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nis evident from Table 1. In all three scenarios, our model does not require real-time generation to meet online recommendation needs. This is particularly significant for scenarios like short-video recom- mendations, where many new items appear daily, further reducing the time delay in system online services.\\n\\n5 ONLINE SERVICE OF LLM-CF In this section, we introduce the online service of LLM-CF in detail.\\n\\n5.1 Overview The online service part of LLM-CF includes the following compo- nents:\\n\\nIn-context CoT Examples Retrieval: The Retrieval process involves finding the top-𝐾 historical recommendation examples similar to the current recommendation data in order to provide explicit collaborative filtering information. This involves identify- ing I𝑖 = {(x𝑘, 𝑐𝑘, 𝑦𝑘 )}𝐾 𝑘=1 from C, which includes recommendation data similar to the current recommendation data, as well as CoT rea- soning 𝑐𝑘 containing world-knowledge and collaborative filtering information.\\n\\nIn-context Chain of Thought (ICT) Module: Inspired by the success of in-context learning and chain of thought in LLMs, we use I𝑖 as in-context CoT examples and x𝑖 as the query. By employing a transformer decoder layer for In-context Chain of Thought learning, we learnt the world-knowledge and reasoning guided collaborative filtering feature w𝑖 , which can be used to enhance underlying recommendation models in a model-agnostic manner.\\n\\nTraining: During the training phase, we designed a reconstruc- tion loss for the CoT reasoning in the in-context examples, to fur- ther strengthen the world-knowledge and reasoning capabilities contained in the collaborative filtering features generated by ICT module.\\n\\n5.2 In-context CoT Examples Retrieval The Retrieval module is responsible for retrieving similar In-context CoT examples for the current recommendation data (x𝑖, 𝑦𝑖 ). Re- cent studies in retrieval-augmented recommendation have shown the potential of using current recommendation features (i.e., user features and target item features) for history retrieval to enhance collaborative filtering capabilities [3, 31, 32]. Our approach extends this concept by not only leveraging the collaborative filtering infor- mation but also incorporating the world knowledge and reasoning abilities of RecGen-LLaMA.\\n\\nWe use the text format features x𝑡\\n\\n𝑖 of x𝑖 as the query to retrieve similar In-context CoT examples from the In-context CoT dataset C, where the key for the examples in C is composed of the text format features: K = [x𝑡 1\\n\\n𝑚, . . . , x𝑡 We employed embedding-based retrieval, including the encoding and ranking processes. To implement this process efficiently, the retrieval process is based on the Approximate Nearest Neighbor search [27]. In the Encoding phase, we used the BGE embedding [47] as the text encoder to convert the query x𝑡 𝑖 and the candidate keys of the In-context CoT dataset K into embedding formats:\\n\\n, . . . , x𝑡\\n\\n𝑀 ].\\n\\n𝑡 𝑡 𝑖 ), encoder(K), 𝑖 ), e(K) = encoder(x e(x\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nwhere e(K) = [e(x𝑡\\n\\n𝑚), . . . , e(x𝑡 In the ranking phase, the cosine similarity between the query 𝑖 ) and each candidate key’s embedding in e(K) is 𝑚) in e(K), the\\n\\n1), . . . , e(x𝑡\\n\\n𝑀 )].\\n\\nembedding e(x𝑡 computed. Specifically, for each embedding e(x𝑡 cosine similarity with e(x𝑡\\n\\n𝑖 ) is calculated as follows: e(x𝑡 ∥e(x𝑡\\n\\n𝑖 ) · e(x𝑡 𝑚) 𝑖 )∥ ∥e(x𝑡 𝑚)∥\\n\\n𝑡 𝑡 𝑖 ), e(x 𝑚)) = sim(e(x\\n\\nwhere · denotes the dot product and ∥ · ∥ denotes the norm of a vector. Subsequently, the indices of the top-𝐾 most similar candidate keys are identified based on their cosine similarity scores. These indices correspond to the examples in the In-context CoT dataset C. The In-context CoT examples associated with these indices are extracted from C, forming:\\n\\nI𝑖 = {E1, . . . , E𝑘, . . . , E𝐾 } , where E𝑘 = (x𝑘, 𝑐𝑘, 𝑦𝑘 ). The I𝑖 contains the world-knowledge and reasoning enhanced explicitly collaborative filtering information of x𝑖 .\\n\\nRemark. (1) To prevent data leakage, we ensure that I𝑖 will def- initely not contain interaction information from future time steps of x𝑖 . (2) To ensure that the proportion of positive and negative labels in I𝑖 does not affect the prediction results of downstream recommendation tasks, we constrain the ratio of positive and neg- ative labels in I𝑖 to remain equal, thereby blocking shortcuts and highlighting the effectiveness of our model.\\n\\n5.3 In-context Chain of Thought Module Inspired by the success of in-context learning and chain of thought in LLMs, the In-context Chain of Thought (ICT) Module learns world knowledge and reasoning-guided Collaborative Filtering fea- tures through an in-context chain of thought methodology.\\n\\nThe ICT module utilizes I𝑖 as in-context examples and x𝑖 as the\\n\\nquery, forming the ICT tokens:\\n\\nT = [x1, 𝑐1, 𝑦1, ...., x𝐾 , 𝑐𝐾 , 𝑦𝐾 , x𝑖 ].\\n\\nThe ICT module first encodes the recommendation features (i.e., user features and target item features), CoT reasoning, and label of T into corresponding tokens embedding E:\\n\\nE = [r1, c1, l1, ...., r𝐾 , c𝐾 , l𝐾 , r𝑖 ].\\n\\nThe high-dimensional sparse one-hot recommendation features in x are mapped into low-dimensional dense space via an ID Embedding layer with an embedding lookup operation, and the text features of x are encoded by the text encoder (the same as the In-context CoT examples Retrieval), then the underlying recommendation model’s feature encoding module encodes them into the token r. The CoT reasoning 𝑐 is also mapped into low-dimensional dense space via the text encoder and MLP projection, forming the token c. The label 𝑙 is the binary one-hot vector, which can also be mapped into a low-dimensional dense space via the ID embedding layer to obtain l.\\n\\nConsidering existing research indicating a correlation between the in-context learning capability and the structural attributes of transformer decoders [7, 35], we use transformer decoder layers to encode E. The ICT token embedding E is then fed through 𝐿\\n\\n(2)\\n\\n(3)\\n\\nZhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu\\n\\nTransformer Decoder blocks, generating hidden representations H of ICT tokens:\\n\\nH = [h(r1), h(c1), h(l1), · · · , h(r𝑖 ))] = Decoder(E). The world knowledge and reasoning guided collaborative filtering feature w is the next token of T (i.e., the last hidden representations of H ):\\n\\nw = h(r𝑖 ). Please note that the length of decoder input is not long in our experiments. This ensures that the ICT module does not impose much time overhead on the recommendation models. We leave details in Section 5.6.\\n\\n5.4 Model-Agnostic Application The primary goal of recommendation models is to learn a function 𝑓 (·) characterized by parameters 𝜃 , which can skillfully predict the click probability 𝑃 (𝑦𝑖 = 1|x𝑖 ) for each sample x𝑖 , formalized as ˆ𝑦𝑖 = 𝑓 (x𝑖 ; 𝜃 ). The world-knowledge and reasoning guided collaborative filtering feature w𝑖 can be directly utilized in enhanced underlying recommendation models:\\n\\nˆ𝑦𝑖 = 𝑓 ([x𝑖, w𝑖 ]; 𝜃 ).\\n\\nFor ranking models, there is a greater need for interactions between recommendation features; here, [·, ·] can be a concatenation opera- tion, followed by the use of a feature interaction module to learn deep interactions. In contrast, retrieval models emphasize efficiency more, and [·, ·] can be an addition operation.\\n\\n5.5 Model Training In the model training phase, for each data (x𝑖, 𝑦𝑖 ) ∈ D, in addition to the original loss of the underlying recommendation model L𝑖 𝑜 , we also designed a reconstruction loss for the CoT reasoning in the In- context CoT Examples to further strengthen the world-knowledge and reasoning capabilities contained in the collaborative filtering features w𝑖 . The reconstruction loss is to minimize the distance between the predicted embedding and the ground-truth embedding of CoT reasoning:\\n\\nL𝑖\\n\\n𝑟 =\\n\\n1 𝐾\\n\\n𝐾 ∑︁\\n\\n𝑖=1\\n\\n(1 −\\n\\nc𝑖 · h(r𝑖 ) ∥c𝑖 ∥∥h(r𝑖 )∥\\n\\n),\\n\\nwhere 𝐾 is the number of In-context CoT Examples.\\n\\nFinally, the total loss L is computed as:\\n\\nL =\\n\\n1 𝑁\\n\\n𝑁 ∑︁\\n\\n𝑖=1\\n\\n(𝛼 L𝑖\\n\\n𝑟 + L𝑖\\n\\n𝑜 ),\\n\\nwhere 𝛼 are hyperparameters that control the importance of the two parts of the loss, and 𝑁 is the number of recommendation data.\\n\\n5.6 Efficiency Analysis of Online Service In the online service of LLM-CF, as shown in Figure 3, only the\\n\\nblue fire module requires online computation. The ICT module’s inputs are short sequences that need minimal computational over- head. Experiments (§ 6.5) show that the number of ICT examples only needs 𝐾 = 4, meaning the sequence length of ICT tokens T is only a dozen or so. Furthermore, it can utilize other acceleration techniques for transformer decoders, which are currently being\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nTable 2: Basic statistics of datasets.\\n\\nDataset\\n\\nSports Beauty\\n\\nToys\\n\\n#Users #Items #Reviews #Sparsity (%)\\n\\n35,598 18,357 296,337 0.0453\\n\\n22,363 12,101 198,502 0.0734\\n\\n19,412 11,924 167,597 0.0724\\n\\nwidely studied [22]. The computation time of the recommenda- tion model is essentially the same as that of underlying models, and the inputs ICT token embedding for the ICT module r and the final underlying model are only needed to compute once, re- quiring no additional computational overhead. Given the reduced necessity for maintaining an extensive In-Context CoT Dataset, the retrieval process for In-Context CoT Examples can be efficiently accelerated through the utilization of precomputed embedding vec- tors in conjunction with Approximate Nearest Neighbor search algorithms [27, 34].\\n\\nThe analysis of online service efficiency for LLM-CF, combined with the offline efficiency analysis in § 4.4, demonstrates the feasi- bility of our framework for deployment.\\n\\n6 EXPERIMENT In this section, we empirically verify the effectiveness of LLM-CF by extensive experiments. The experimental details and source codes can be found at (https://anonymous.4open.science/r/LLM- CF-AD78).\\n\\n6.1 Experimental Setup 6.1.1 Datasets. We conducted experiments on three widely used datasets with varying domains, following existing work [5, 11, 26, 54, 55]. Their statistics are reported in Table 2.\\n\\nThe Amazon1 review datasets [14] are one of the most widely used benchmarks for recommendation. We adopt three subsets of them: Sports & Outdoors, Beauty, and Toys & Games. Following the common practices [11, 26, 54, 55], we use the reviews between January 1, 2019, to December 31, 2019 and treat all the recorded reviews as positive samples.\\n\\n6.1.2 Baselines. To validate the versatility of LLM-CF on the rank- ing phase, we selected representative CTR models as backbone models. CTR prediction is commonly applied in the ranking phase of recommendation systems, focusing on modeling the probability of users clicking on items through feature interactions. The selected six backbone models are as follows: DeepFM [12]. xDeepFM [23], AutoInt [37], DCNv1 [40], DCNv2 [41], and DIN [53].\\n\\nTo validate the effectiveness of LLM-CF, we chose representative methods that leverage LLMs to enhance recommendation models as the competitive framework. Considering that our framework initially fine-tunes LLaMA2, and LLaMA2 performs well on CTR tasks, a natural idea is to employ knowledge distillation using fine- tuned LLaMA2 to enhance CTR models. Following the previous methods [17, 56], we implemented the KD framework, which uti- lizes the logits of fine-tuned LLaMA2 as soft labels for CTR models during training. We also compared our framework with a recent\\n\\n1http://jmcauley.ucsd.edu/data/amazon/\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nwork, KAR [46], which utilizes the open-world knowledge of LLMs to enhance recommendations. For a fair comparison, we used the same LLM (i.e., LLaMA2-7B-chat) as other models in KAR.\\n\\n6.1.3 Evaluation Metrics & Protocal. Following the common prac- tices [11, 26, 54, 55], we use the leave-one-out strategy to partition the training, validation, and test datasets. We consider all recorded interactions in the datasets as positive samples and randomly sam- ple a negative sample for each positive sample. For the evaluation metrics, we employ widely used AUC (Area under the ROC curve), LogLoss (binary cross-entropy loss), and RelaImpr (the relative im- provement of AUC) [53] for the CTR prediction task.\\n\\nImplementation Details. As for the LLM, we utilized the 6.1.4 widely used LLaMA2-7B-chat [39]. To finetune LLaMA2, we em- ployed the previously mentioned strategy of mixing recommenda- tion data and general data (i.e., RecGen in § 4.2). We use 8 Nvidia A800 GPUs to perform full-parameter SFT on the LLM. We adopt the DeepSpeed zero-2 strategy [33], where each GPU has a batch size of 16, resulting in a total batch size of 128 without gradient accu- mulation. We used the Adam optimizer and set the initial learning rate as 1e-5 with the cosine learning rate schedule.\\n\\nFor the application of LLM-CF to backbone models, we trained them using the Adam optimizer with a learning rate of 0.001 and a batch size of 128. Additionally, we adopted early-stopped training to avoid over-fitting. The dimension of all feature embeddings is set to 32 for all methods. The number of layers and heads in the ICT module’s transformer decoder is set to 2. The number of In-context CoT Dataset 𝑀 is 1 10 of the Training Dataset. The number of In-context CoT examples 𝐾 is set to 4. The 𝛼 is tuned among (0, 1] in the step of 0.1. Given the fact that LLM-CF and competitive framework are applied over backbone models, for a fair comparison, we maintain consistency of the backbone models across all frameworks. For instance, the backbone parts of LLM-CF (xDeepFM), KD(xDeepFM), and KAR(xDeepFM) share the same model architectures and basic hyper-parameters as xDeepFM.\\n\\n6.2 Experimental Results Table 3 reports the overall performance of LLM-CF over 6 backbone models on three real-world datasets. Based on the results presented in Table 3, we found that LLM-CF outperformed the corresponding backbone models in terms of all evaluation metrics on real-world datasets, with statistical significance. The results verified the ef- fectiveness of the LLM-CF, which can effectively integrate world- knowledge and reasoning-guided collaborative filtering features into the underlying recommendation models using the in-context chain of thought manner.\\n\\nWe also observed that LLM-CF achieved better results than other frameworks that leverage LLMs to enhance recommendation mod- els (i.e., KAR and KD). Compared with LLM-CF, KAR overlooks the importance of collaborative filtering information, only considering current user and item features to enhance recommendation models, and does not use an LLM fine-tuned with recommendation data, leading to a significant performance drop in real-world in-domain datasets used in our experiments. The suboptimal performance of KD is mainly due to the fact that the training objective of LLMs is\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nZhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu\\n\\nTable 3: Overall performance comparisons. The best and the second-best performance methods are denoted in bold and underlined fonts, respectively. * means improvements over the second-best methods are significant (p-value < 0.05). The RelaImpr is calculated over backbone models, i.e., ‘None’ framework. To make a fair comparison, KD, KAR, and LLM-CF are all based on the 7B-parameter LLaMA2 model.\\n\\nBackbone\\n\\nFramework\\n\\nSports\\n\\nBeauty\\n\\nToys\\n\\nAUC↑\\n\\nLogloss↓ RelaImpr↑\\n\\nAUC↑\\n\\nLogloss↓ RelaImpr↑\\n\\nAUC↑\\n\\nLogloss↓ RelaImpr↑\\n\\nDeepFM\\n\\nxDeepFM\\n\\nAutoInt\\n\\nDCNv1\\n\\nDCNv2\\n\\nNone KD KAR LLM-CF None KD KAR LLM-CF None KD KAR LLM-CF None KD KAR LLM-CF None KD KAR LLM-CF None KD KAR LLM-CF\\n\\n0.7990 0.8043 0.7991 0.8137∗ 0.8158 0.8169 0.8161 0.8196∗ 0.8003 0.8012 0.8039 0.8088∗ 0.8023 0.8040 0.8024 0.8092∗ 0.8110 0.8112 0.8087 0.8131∗ 0.7986 0.8023 0.7971 0.8089∗\\n\\n0.5471 0.5404 0.5469 0.5306∗ 0.5318 0.5298 0.5279 0.5248∗ 0.5444 0.5439 0.5390 0.5391 0.5442 0.5441 0.5469 0.5368∗ 0.5331 0.5320 0.5363 0.5307∗ 0.5519 0.5422 0.5525 0.5374∗\\n\\n0.000% 1.773% 0.033% 4.916% 0.000% 0.348% 0.094% 1.203% 0.000% 0.300% 1.199% 2.831% 0.000% 0.562% 0.033% 2.282% 0.000% 0.064% -0.739% 0.675% 0.000% 1.239% -0.502% 3.449%\\n\\n0.7853 0.7959 0.7870 0.8044∗ 0.8065 0.8104 0.8101 0.8113 0.7949 0.7961 0.7939 0.8090∗ 0.8146 0.8147 0.8165 0.8182∗ 0.8028 0.8057 0.8003 0.8033 0.7861 0.7934 0.7861 0.7967∗\\n\\n0.5545 0.5442 0.5546 0.5366∗ 0.5359 0.5345 0.5315 0.5311 0.5469 0.5444 0.5476 0.5321∗ 0.5255 0.5286 0.5229 0.5216∗ 0.5378 0.5343 0.5404 0.5372 0.5613 0.5518 0.5604 0.5492∗\\n\\n0.000% 3.715% 0.596% 6.695% 0.000% 1.272% 1.175% 1.566% 0.00% 0.407% -0.339% 4.781% 0.000% 0.031% 0.604% 1.144% 0.00% 0.958% -0.825% 0.165% 0.000% 2.551% 0.000% 3.705%\\n\\n0.7681 0.7713 0.7698 0.7881∗ 0.7836 0.7865 0.7898 0.7947∗ 0.7630 0.7635 0.7683 0.7754∗ 0.7621 0.7652 0.7651 0.7702∗ 0.7774 0.7827 0.7759 0.7812 0.7586 0.7652 0.7620 0.7783∗\\n\\n0.5770 0.5716 0.5718 0.5581∗ 0.5589 0.5553 0.5529 0.5473∗ 0.5770 0.5770 0.5741 0.5685∗ 0.5831 0.5847 0.5821 0.5745∗ 0.5650 0.5609 0.5662 0.5619 0.5885 0.5847 0.5874 0.5699∗\\n\\n0.000% 1.194% 0.634% 7.460% 0.000% 1.023% 2.186% 3.914% 0.000% 0.190% 2.015% 4.714% 0.000% 1.183% 1.144% 3.090% 0.000% 1.911% -0.541% 1.370% 0.000% 2.552% 1.315% 7.618%\\n\\nDIN\\n\\nTable 4: Overall performance of LLM-CF on retrieval tasks. The best performance methods are denoted in bold.\\n\\nModels\\n\\nBeauty\\n\\nSports\\n\\nToys\\n\\nHIT@5 HIT@10 NDCG@5 NDCG@10 HIT@5 HIT@10 NDCG@5 NDCG@10 HIT@5 HIT@10 NDCG@5 NDCG@10 0.0379 SASREC 0.0395 +LLM-CF YoutubeDNN 0.0150 0.0160 +LLM-CF 0.0279 GRU4REC 0.0303 +LLM-CF 0.0199 SRGNN 0.0214 +LLM-CF\\n\\n0.0648 0.0652 0.0272 0.0291 0.0470 0.0504 0.0341 0.0359\\n\\n0.0206 0.0210 0.0089 0.0099 0.0180 0.0186 0.0123 0.0128\\n\\n0.0293 0.0293 0.0128 0.0139 0.0241 0.0250 0.0169 0.0174\\n\\n0.0161 0.0205 0.0109 0.0112 0.0165 0.0203 0.0085 0.0118\\n\\n0.0203 0.0349 0.0177 0.0191 0.0268 0.0317 0.0146 0.0210\\n\\n0.0088 0.0107 0.0069 0.0069 0.0102 0.0127 0.0054 0.0072\\n\\n0.0101 0.0154 0.0091 0.0094 0.0135 0.0164 0.0073 0.0101\\n\\n0.0329 0.0466 0.0127 0.0165 0.0257 0.0220 0.0158 0.0177\\n\\n0.0646 0.0723 0.0229 0.0285 0.0411 0.0380 0.0284 0.0302\\n\\n0.0143 0.0238 0.0077 0.0099 0.0152 0.0139 0.0096 0.0104\\n\\n0.0251 0.0321 0.0110 0.0138 0.0202 0.0192 0.0137 0.0144\\n\\nCross-Entropy for all tokens, while the underlying recommenda- tion training mainly involves Binary Cross-Entropy loss, resulting in a misalignment of optimization goals. The results verify that LLM-CF can better leverage LLMs to provide enhanced collabora- tive filtering information to underlying recommendation models.\\n\\nfor LLM-CF. They are: (a) w/o text: Remove the text features in recommendation features. (b) w/o CoT: Remove the CoT reasoning along with reconstruction loss in in-context CoT examples. (c) w/o RecGen-LLaMA: Replace the RecGen-LLaMA with the original LLaMA2-7B-chat to generate CoT reasoning. (d) w/o TF decoder: Replace the transformer decoder layers with mean pooling in the ICT module.\\n\\n6.3 Ablation Study LLM-CF consists of several key operations, and to understand the ef- fects of each operation, we conducted several ablation experiments\\n\\nTable 5 presents the performance of various model variants over 6 backbone models on three real-world datasets. The results reveal\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nn4\\n\\nBeauty\\n\\nToys\\n\\nn8\\n\\nn0\\n\\n0.82AUC\\n\\n0.80AUC\\n\\nn4, 0.8033n4, 0.8131n8, 0.7868\\n\\nn8\\n\\nSports\\n\\nn2\\n\\nToys\\n\\n0.80\\n\\n0.80\\n\\nn4_nocxDeepFM\\n\\nn4_nocDIN\\n\\nn8\\n\\nn4_nocDCNv1\\n\\n0.78\\n\\nToys\\n\\nn8\\n\\n0.78\\n\\nn2\\n\\nn4\\n\\n0.80\\n\\n0.82AUC\\n\\nn4\\n\\nn4_nocDCNv2\\n\\nToys\\n\\nn8\\n\\n0.80\\n\\n0.76\\n\\nToys\\n\\nn2\\n\\n0.78\\n\\nn4_nocDeepFM\\n\\nn4, 0.8113n4_noc, 0.8208n8, 0.795\\n\\nToys\\n\\nn6\\n\\nn4\\n\\nn6\\n\\nn4_nocAutoInt\\n\\nBeauty\\n\\nBeauty\\n\\nn2\\n\\nn0\\n\\nBeauty\\n\\n0.78\\n\\nn0\\n\\n0.76\\n\\nSports\\n\\nSports\\n\\nn8\\n\\n0.76\\n\\nn0\\n\\n0.82AUC\\n\\nn6\\n\\nBeauty\\n\\nn6, 0.8065n4_noc, 0.8158n8, 0.7894\\n\\nn6, 0.797n4, 0.8089n2, 0.7796\\n\\n0.80AUC\\n\\nn2\\n\\nBeauty\\n\\nn4, 0.809n4, 0.8088n4, 0.7754\\n\\nn0\\n\\nn6\\n\\nn0\\n\\nn4\\n\\nn6\\n\\n0.82AUC\\n\\n0.76\\n\\nn4\\n\\nSports\\n\\n0.76\\n\\nn6\\n\\n0.78\\n\\nSports\\n\\nSports\\n\\n0.78\\n\\nn4, 0.8182n4, 0.8092n4_noc, 0.772\\n\\nn2\\n\\nFigure 5: The recommendation performance of LLM-CF on 6 backbone models across three real-world datasets as it varies with the change in In-context CoT examples length and positive/negative example constraints. The red dots indicate the best-performing setting for the current backbone model and dataset.\\n\\nTable 5: Ablation Study of LLM-CF. The numbers in boldface indicate the best results.\\n\\nBackbone\\n\\nFramework\\n\\nSports\\n\\nBeauty\\n\\nToys\\n\\nAUC↑\\n\\nLogloss↓ AUC↑\\n\\nLogloss↓ AUC↑\\n\\nLogloss↓\\n\\nDeepFM\\n\\nxDeepFM\\n\\nAutoInt\\n\\nDCNv1\\n\\nDCNv2\\n\\n0.7990 None 0.8137 w/o text w/o CoT 0.8009 w/o RecGen-LLaMA 0.8132 0.8071 w/o TF decoder 0.8137 LLM-CF 0.8158 None 0.8200 w/o text w/o CoT 0.8177 w/o RecGen-LLaMA 0.8191 0.8192 w/o TF decoder 0.8196 LLM-CF 0.8003 None 0.8071 w/o text w/o CoT 0.7992 w/o RecGen-LLaMA 0.8041 0.8017 w/o TF decoder 0.8088 LLM-CF 0.8023 None 0.8099 w/o text w/o CoT 0.8017 w/o RecGen-LLaMA 0.8036 w/o TF decoder 0.8006 0.8092 LLM-CF 0.8110 None 0.8130 w/o text 0.8081 w/o CoT w/o RecGen-LLaMA 0.8117 0.8113 w/o TF decoder 0.8131 LLM-CF 0.7986 None 0.8053 w/o text w/o CoT 0.8025 w/o RecGen-LLaMA 0.8057 0.8004 w/o TF decoder 0.8089 LLM-CF\\n\\n0.5471 0.5308 0.5448 0.5301 0.5378 0.5306 0.5318 0.5248 0.5290 0.5246 0.5246 0.5248 0.5444 0.5380 0.5468 0.5398 0.5420 0.5391 0.5442 0.5408 0.5441 0.5432 0.5465 0.5368 0.5331 0.5297 0.5365 0.5313 0.5314 0.5307 0.5519 0.5415 0.5445 0.5410 0.5484 0.5374\\n\\n0.7853 0.8030 0.7980 0.7973 0.7986 0.8044 0.8065 0.8088 0.8081 0.8111 0.8092 0.8113 0.7949 0.8043 0.7940 0.7993 0.7942 0.8090 0.8146 0.8155 0.7622 0.7891 0.8118 0.8182 0.8028 0.7971 0.7968 0.8023 0.7978 0.8033 0.7861 0.7938 0.7907 0.7953 0.7850 0.7967\\n\\n0.5545 0.5382 0.5423 0.5452 0.5411 0.5366 0.5356 0.5308 0.5349 0.5311 0.5349 0.5311 0.5468 0.5385 0.5490 0.5412 0.5474 0.5321 0.5255 0.5338 0.5802 0.5617 0.5294 0.5216 0.5378 0.5447 0.5457 0.5381 0.5437 0.5372 0.5613 0.5509 0.5557 0.5571 0.5602 0.5492\\n\\n0.7681 0.7880 0.7766 0.7806 0.7739 0.7881 0.7836 0.7898 0.7915 0.7936 0.7912 0.7947 0.7630 0.7710 0.7581 0.7706 0.7592 0.7754 0.7621 0.7692 0.7622 0.7720 0.7598 0.7702 0.7774 0.7790 0.7753 0.7826 0.7810 0.7812 0.7586 0.7742 0.7718 0.7783 0.7666 0.7783\\n\\n0.5770 0.5583 0.5662 0.5640 0.5682 0.5581 0.5589 0.5547 0.5561 0.5490 0.5571 0.5473 0.5770 0.5725 0.5812 0.5702 0.5816 0.5685 0.5831 0.5751 0.5802 0.5733 0.5858 0.5745 0.5650 0.5628 0.5681 0.5602 0.5612 0.5619 0.5885 0.5731 0.5735 0.5737 0.5794 0.5699\\n\\nDIN\\n\\nseveral insights: (1) After removing CoT reasoning, there is a signif- icant decline in recommendation performance, demonstrating the effectiveness of CoT reasoning in providing world knowledge and\\n\\nreasoning for collaborative filtering features. (2) Replacing RecGen- LLaMA with the original LLaMA2-7B-chat to generate CoT rea- soning shows a downward trend in recommendation performance, highlighting the necessity of enhancing the recommendation capa- bilities of LLM when used to augment conventional recommenda- tion models. (3) After replacing the transformer decoder layers with mean pooling in the ICT module, there is a noticeable decline in rec- ommendation performance, underscoring the importance of using transformer decoder layers for learning in-context CoT examples. (4) Removing text features leads to a slight decline in performance, but the change is not significant, suggesting that text features have a limited role in the framework.\\n\\n6.4 Application of Retrieval Tasks LLM-CF is versatile, applicable not only to models in the ranking stage but also to models in the retrieval stage. In the preceding exper- iments, we validated the efficacy of LLM-CF in the CTR prediction task. In this section, we employed LLM-CF over retrieval models (SASREC [20], YoutubeDNN [6], GRU4REC [16] and SRGNN [45]) and experimentally validated its effectiveness on four ranking met- rics. We trained the models using the sampled softmax loss, with 128 randomly sampled negatives for each interaction. Since these models use inner product to estimate probabilities, when applying LLM-CF, we fused the vectors obtained from LLM-CF with the user vector in the backbone model by element-wise addition. To prevent data leakage, for the retrieval task, we mask the information of the target item in the current interaction in the in-context chain of thought learning. Table 4 reports the overall performance of LLM- CF over 4 backbone models on three real-world datasets. LLM-CF enhances the performance of backbone retrieval models, further validating the versatility of the collaborative filtering features en- hanced by world knowledge and reasoning.\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\n6.5 Effects of In-context CoT Examples Considering the length of In-context CoT (ICT) examples and the ratio of positive to negative examples has an important impact on the final prediction results. We examine the influence of the length of ICT examples and the effect of relaxing positive/negative (P/N) example ratio constraints on the performance of the LLM-CF across three real-world datasets and six backbone models.\\n\\nAs shown in Figure 5, ICT lengths were varied across 0 (n0), 2 (n2), 4 (n4), 6 (n6), and 8 (n8) with a consistent P/N sample ratio. Our empirical results reveal that n4 consistently outperforms n2, while lengths of n6 and n8 show variable performance, occasionally surpassing but generally not exceeding n4. A notable performance decline for most datasets and models at n8 suggests a diminishing return with longer ICT, positing n4 as an effective balance between performance gains and computational efficiency.\\n\\nFurthermore, the removal of P/N ratio constraints (denoted as 𝑛4_noc) shown in Figure 5 did not significantly deviate from the performance of the constrained scenarios. While some datasets showed marginal improvements without P/N constraints, others exhibited a decrease. Maintaining an equal P/N sample ratio ef- fectively validated the superiority of LLM-CF, reducing potential biases arising from disproportionate label distributions.\\n\\n7 CONCLUSION In this paper, we proposed the Large Language Model enhanced Collaborative Filtering (LLM-CF) Framework, a novel approach that integrates the world knowledge and reasoning capabilities of Large Language Models into Recommender Systems. LLM-CF lever- ages LLMs’ world knowledge and reasoning, particularly through the In-context Chain of Thought module, to enhance collaborative filtering in RSs. The key contribution of LLM-CF is its effective distillation of LLM capabilities into RSs, balancing recommendation accuracy with operational efficiency. Our experiments across vari- ous datasets demonstrate that LLM-CF significantly outperforms conventional recommendation models in both ranking and retrieval tasks, confirming the benefits of integrating LLMs into RSs using LLM-CF.\\n\\nREFERENCES [1] Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and S. Gupta. 2020. Better Fine-Tuning by Reducing Representa- tional Collapse. ArXiv abs/2008.03156 (2020).\\n\\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. Proceedings of the 17th ACM Conference on Recommender Systems (2023).\\n\\n[3] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, and Kun Gai. 2023. TWIN: TWo- stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou. Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2023).\\n\\n[4] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A survey of chain of thought reasoning: Advances, frontiers and future. arXiv preprint arXiv:2309.15402 (2023).\\n\\n[5] Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, James Y Zhang, and Sheng Li. 2023. Leveraging Large Language Models for Pre-trained Recommender Systems. arXiv:2308.10837 [cs.IR]\\n\\n[6] Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. Proceedings of the 10th ACM Conference on Recommender Systems (2016).\\n\\nZhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu\\n\\n[7] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers. In Findings of the Association for Computa- tional Linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, 4005–4019.\\n\\n[8] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, ZhongX- iang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT’s Capabilities in Recommender Systems. Proceedings of the 17th ACM Conference on Recommender Systems (2023). https://api.semanticscholar.org/CorpusID:258461170\\n\\n[9] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayi- heng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. ArXiv abs/2310.05492 (2023).\\n\\n[10] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n\\n[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In Proceedings of the 16th ACM Conference on Rec- ommender Systems (Seattle, WA, USA) (RecSys ’22). Association for Computing Ma- chinery, New York, NY, USA, 299–315. https://doi.org/10.1145/3523227.3546767 [12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI’17). AAAI Press, 1725–1731.\\n\\n[13] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, D. Jan- nach, and Marios Fragkoulis. 2023. Leveraging Large Language Models for Sequential Recommendation. Proceedings of the 17th ACM Conference on Recom- mender Systems (2023). https://api.semanticscholar.org/CorpusID:261823711 [14] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Vi- sual Evolution of Fashion Trends with One-Class Collaborative Filtering. In Proceedings of the 25th International Conference on World Wide Web (Mon- tréal, Québec, Canada) (WWW ’16). International World Wide Web Confer- ences Steering Committee, Republic and Canton of Geneva, CHE, 507–517. https://doi.org/10.1145/2872427.2883037\\n\\n[15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language under- standing. arXiv preprint arXiv:2009.03300 (2020).\\n\\n[16] Balázs Hidasi and Alexandros Karatzoglou. 2018. Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. In CIKM. ACM, 843–852. [17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in\\n\\na Neural Network. arXiv:1503.02531 [stat.ML]\\n\\n[18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR. OpenReview.net.\\n\\n[19] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. 2023. NEFTune: Noisy Embeddings Improve Instruction Finetuning. arXiv preprint arXiv:2310.05914 (2023).\\n\\n[20] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom- mendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197–206.\\n\\n[21] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed H. Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. ArXiv abs/2305.06474 (2023). https://api.semanticscholar.org/CorpusID:258615591\\n\\n[22] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2019. Reformer: The Efficient\\n\\nTransformer. In International Conference on Learning Representations.\\n\\n[23] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. XDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD ’18). Association for Computing Machinery, New York, NY, USA, 1754–1763. https://doi.org/10.1145/3219819.3220023\\n\\n[24] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. arXiv preprint arXiv:2308.11131 (2023).\\n\\n[25] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2023. A First arXiv preprint\\n\\nLook at LLM-Powered Generative News Recommendation. arXiv:2305.06566 (2023).\\n\\n[26] Kai Mei and Yongfeng Zhang. 2023. LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. arXiv:2310.17488 (2023). [27] Marius Muja and David G. Lowe. 2009. Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration. In International Conference on Computer Vision Theory and Applications.\\n\\n[28] Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large Language Model Augmented Narrative Driven Recommendations. Proceedings of the 17th\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nACM Conference on Recommender Systems (2023).\\n\\n[29] OpenAI. 2023. GPT-4 Technical Report. ArXiv abs/2303.08774 (2023). https:\\n\\n//api.semanticscholar.org/CorpusID:257532815\\n\\n[30] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.\\n\\nInstruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 (2023).\\n\\n[31] Qi Pi, Xiaoqiang Zhu, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, and Kun Gai. 2020. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. Proceedings of the 29th ACM International Conference on Information & Knowledge Management (2020).\\n\\n[32] Jiarui Qin, Weinan Zhang, Rong Su, Zhirong Liu, Weiwen Liu, Ruiming Tang, Xiuqiang He, and Yong Yu. 2021. Retrieval & Interaction Machine for Tabular Data Prediction. Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (2021).\\n\\n[33] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: Inter- national Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–16.\\n\\n[34] Jie Ren, Minjia Zhang, and Dong Li. 2020. HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS’20). Curran Associates Inc., Red Hook, NY, USA, Article 895, 13 pages.\\n\\n[35] Ruifeng Ren and Yong Liu. 2023. In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern. arXiv preprint arXiv:2310.13220 (2023).\\n\\n[36] Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, and Lucas Dixon. 2023. Large language models are competitive near cold-start recommenders for language-and item-based preferences. In Proceedings of the 17th ACM conference on recommender systems. 890–896.\\n\\n[37] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management (Beijing, China) (CIKM ’19). Association for Computing Machinery, New York, NY, USA, 1161–1170. https://doi.org/10.1145/3357384.3357925\\n\\n[38] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023).\\n\\n[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas- mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos- ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\\n\\n[40] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD’17 (Halifax, NS, Canada) (ADKDD’17). Association for Computing Machinery, New York, NY, USA, Article 12, 7 pages. https://doi.org/10.1145/3124749.3124754\\n\\n[41] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW ’21). Association for Computing Machinery, New York, NY, USA, 1785–1797. https://doi.org/10.1145/3442381.3450078 [42] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022).\\n\\n[43] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023. LLMRec: Large Language Models with Graph Augmentation for Recommendation. ArXiv (2023).\\n\\n[44] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Korn- blith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. 2022. Robust Fine-Tuning of Zero- Shot Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 7959–7971.\\n\\n[45] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019. Session-Based Recommendation with Graph Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence 33, 01 (July 2019), 346–353. https: //doi.org/10.1609/aaai.v33i01.3301346\\n\\n[46] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recom- mendation with Knowledge Augmentation from Large Language Models. arXiv preprint arXiv:2306.10933 (2023).\\n\\n[47] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL]\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\n[48] Bin Yin, Jun Xie, Yu-Xia Qin, Zixiang Ding, Zhichao Feng, Xiang Li, and Wei Lin. 2023. Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Rec- ommendation via LLM. Proceedings of the 17th ACM Conference on Recommender Systems (2023). https://api.semanticscholar.org/CorpusID:260682982\\n\\n[49] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. 2022. HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Represen- tation Perturbation. In Annual Meeting of the Association for Computational Linguistics.\\n\\n[50] Zhenrui Yue, Sara Rabhi, Gabriel De Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023. LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking. ArXiv abs/2311.02089 (2023). https://api.semanticscholar. org/CorpusID:265033634\\n\\n[51] Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, and Hui Xiong. 2023. Generative job recommendations with large language model. arXiv preprint arXiv:2307.02157 (2023).\\n\\n[52] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 (2023).\\n\\n[53] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click- Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD ’18). Association for Computing Machinery, New York, NY, USA, 1059–1068. https://doi.org/10.1145/3219819.3219823\\n\\n[54] Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In CIKM ’20: The 29th ACM International Conference on Information and Knowledge Man- agement, Virtual Event, Ireland, October 19-23, 2020. ACM, 1893–1902.\\n\\n[55] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-Enhanced MLP is All You Need for Sequential Recommendation. In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW ’22). Association for Computing Machinery, New York, NY, USA, 2388–2399. https://doi.org/10.1145/ 3485447.3512111\\n\\n[56] Jieming Zhu, Jinyang Liu, Weiqi Li, Jincai Lai, Xiuqiang He, Liang Chen, and Zibin Zheng. 2020. Ensembled CTR Prediction via Knowledge Distillation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (Virtual Event, Ireland) (CIKM ’20). Association for Computing Machinery, New York, NY, USA, 2941–2958. https://doi.org/10.1145/3340531. 3412704'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBkdOYW_I4XO"
      },
      "source": [
        "# **SPLITTING DOCUMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usi2KLhlINqt"
      },
      "outputs": [],
      "source": [
        "text_sp = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
        "chunks = text_sp.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "qZutBmohIlc8",
        "outputId": "c8d9df2c-06b1-4bb6-f636-bc9e56ac15a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4 2 0 2\\n\\nr a\\n\\nM 6 2\\n\\n]\\n\\nR\\n\\nI . s c [\\n\\n1 v 8 8 6 7 1 . 3 0 4 2 : v i X r a\\n\\nLarge Language Models Enhanced Collaborative Filtering\\n\\nZhongxiang Sun Gaoling School of Artificial Intelligence Renmin University of China Beijing, China sunzhongxiang@ruc.edu.cn\\n\\nZihua Si Gaoling School of Artificial Intelligence Renmin University of China Beijing, China zihua_si@ruc.edu.cn\\n\\nXiaoxue Zang Kuaishou Technology Co., Ltd. Beijing, China zangxiaoxue@kuaishou.com\\n\\nKai Zheng Kuaishou Technology Co., Ltd. Beijing, China zhengkai@kuaishou.com\\n\\nYang Song Kuaishou Technology Co., Ltd. Beijing, China yangsong@kuaishou.com\\n\\nXiao Zhang Jun Xu Gaoling School of Artificial Intelligence Renmin University of China Beijing, China {zhangx89,junxu}@ruc.edu.cn\\n\\nABSTRACT Recent advancements in Large Language Models (LLMs) have at- tracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs). Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs. Al- though the extensive world knowledge embedded in LLMs gener- ally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collab- orative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs. In this pa- per, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which dis- tils the world knowledge and reasoning capabilities of LLMs into collaborative filtering. We also explored a concise and efficient instruction-tuning method, which improves the recommendation capabilities of LLMs while preserving their general functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive ex- periments on three real-world datasets demonstrate that LLM-CF significantly enhances several backbone recommendation models and consistently outperforms competitive baselines, showcasing its effectiveness in distilling the world knowledge and reasoning capabilities of LLM into collaborative filtering.\\n\\nCCS CONCEPTS • Information systems → Recommender systems.\\n\\nKEYWORDS Recommender System, Large Language Model, Collaborative Filter- ing\\n\\nACM Reference Format: Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu. 2018. Large Language Models Enhanced Collaborative Filtering. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 11 pages. https://doi.org/XXXXXXX.XXXXXXX\\n\\n1 INTRODUCTION Large Language Models (LLMs) [29, 39] have made rapid advance- ments, showcasing remarkable capabilities [42] in context compre- hension, reasoning, generalization, and modeling world knowledge, and so on. With the advancement of Large Language Models (LLMs), many researchers are focusing on how to utilize LLMs in recom- mendation systems (RSs). Many studies have already applied LLMs to various aspects of RSs, including ranking [50], Click-Through Rate (CTR) prediction [2, 8, 48], sequential recommendation [13], rating prediction [21], and data augmentation [28, 43]. Consider- ing specific methods to utilize LLMs for RSs, current applications can be classified into two categories. (1) LLMs as RSs: LLMs can be directly prompted or be fine-tuned to function as specialized RSs [2, 8, 36, 48, 50]. (2) LLM-enhanced RSs: Based on world knowledge and reasoning abilities, LLM-derived embedding vec- tors and LLM-generated texts can enhance RSs [13, 46, 48, 51].\\n\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, June 03–05, 2018, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX\\n\\nDespite their effectiveness, there are still several challenges to be addressed. LLMs as RSs suffers from low efficiency due to the resource-intensive nature of LLMs, making their practical ap- plication challenging. LLM-enhanced RSs inadequately exploit collaborative filtering information because the LLM can only take a limited number of users and items as inputs. How to better leverage LLMs to provide enhanced collaborative filtering information to existing RSs becomes key in LLM-enhanced RSs.\\n\\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\\n\\nRecGen-LLaMA\\n\\nReasoning andWorld Knowledge\\n\\nCoTRecommendation Features\\n\\nOnline serviceOffline serviceSimilar ExamplesIn-context CoT Dataset\\n\\nRecommendation ModelRetrieveWorld Knowledge and Reasoning guided CF LabelRecommendation Data\\n\\nInstructionTuning+\\n\\nGeneral Data\\n\\nUserItem\\n\\nFigure 1: LLM-CF integrates LLM-based world knowledge and reasoning with collaborative filtering to improve rec- ommendation performance, using LLMs with recommender capability and decoupled latency-free offline generation.\\n\\nConsidering the challenges in deploying LLMs as RSs due to their inherently extensive parameterization, we focus on LLM- enhanced RSs, which are more applicable and flexible for ex- isting RSs. In order to better guide collaborative filtering to en- hance existing RSs with LLMs. Inspired by Chain-of-Thought (CoT) and In-Context Learning [4, 10] in LLMs, we propose a novel Large Language Models enhanced Collaborative Filtering (LLM- CF) Framework, which distils the world knowledge and reasoning capabilities of LLM into collaborative filtering in an in-context, chain of thought methodology. As shown in Figure 1, LLM-CF can be decoupled into two parts: (1) offline service part (§ 4): Fine- tune LLM to enhance its recommendation capabilities, generate CoT reasoning with collaborative filtering information, and construct in-context CoT dataset. (2) online service part (§ 5): Retrieve the in-context CoT examples, learn the world knowledge and reasoning guided Collaborative Filtering (CF) feature, and use this feature to enhance existing RSs.\\n\\nIn the offline service, we perform instruction tuning on LLM to obtain CF information about users and items in the recommenda- tion data. However, our initial findings indicate that full parameter tuning LLMs could result in substantial forgetting of their gen- eral capabilities, as discussed in § 4.2. We leveraged a simple but effective data mixing method to finetune LLaMA2 [39], and success- fully trained a model RecGen-LLaMA, which achieves an optimal balance between general and recommendation capabilities. Then, we use RecGen-LLaMA to generate CoT reasoning for a subset of examples in training data, forming the in-context CoT dataset.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06oiR4vcHgub"
      },
      "source": [
        "# **EMBEDDINGS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxVQnboBJQCa"
      },
      "outputs": [],
      "source": [
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvWafGiEInjI"
      },
      "outputs": [],
      "source": [
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"local-rag\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzHevrxQu145"
      },
      "outputs": [],
      "source": [
        "import pickle as pk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N5r2QSo2lhk"
      },
      "outputs": [],
      "source": [
        "# pk.dump(vector_db,open('vector_db.pkl','wb'))\n",
        "#   # pk.dump(vector_db, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCE-8taCuZLh"
      },
      "source": [
        "# **Retreival**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHECI1jAuYFF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "oheKX8jaxTM0",
        "outputId": "60f4224c-4e15-495f-c33e-45408adeeef7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ChatOllama' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-67469042480c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LLM from Ollama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mistral\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
          ]
        }
      ],
      "source": [
        "#LLM from Ollama\n",
        "llm = ChatOllama(model=\"mistral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGQ1lynwJsaW"
      },
      "outputs": [],
      "source": [
        "query_prompt = PromptTemplate(\n",
        "    input_variables= ['question'],\n",
        "    template= \"You are an AI language model assistant, your task is to generate a response for the question: {question}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My5KYU9PzPdE"
      },
      "outputs": [],
      "source": [
        "retreiver = MultiQueryRetriever.from_llm(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    llm=llm,\n",
        "    prompt=query_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CygA0KV85MYX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seg6qYg0zv5g"
      },
      "outputs": [],
      "source": [
        "# RAG Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sD5TjeLp0fTs",
        "outputId": "2881ea98-8bd4-4ee0-a49e-f9bba7a80351"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'retreiver' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4ded04754d5e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m chain = (\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretreiver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnablePassthrough\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m|\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m|\u001b[0m \u001b[0mStrOutputParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'retreiver' is not defined"
          ]
        }
      ],
      "source": [
        "chain = (\n",
        "    {\"context\": retreiver, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7kLQDP4kF6o"
      },
      "outputs": [],
      "source": [
        "import streamlit\n",
        "def main():\n",
        "  st.title(\"Ollama Chatbot with RAG\")\n",
        "\n",
        "  chat_input = st.text_input(\"Ask your question:\")\n",
        "  if chat_input:\n",
        "    response = chain.run(context=\"\", question=chat_input)\n",
        "    st.write(\"Chatbot Response:\")\n",
        "    st.write(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGEtnRa11DCF"
      },
      "outputs": [],
      "source": [
        "chain.invoke(input(\"> \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yphP8aYthBx"
      },
      "source": [
        "# **PORT FORWARDING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5EtLORLINdza"
      },
      "outputs": [],
      "source": [
        "# !curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "yk2cer4Hpn4G"
      },
      "outputs": [],
      "source": [
        "# Get your ngrok token from your ngrok account:\n",
        "# https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "token=\"2fojkzjyEaSY0fGVPQpPAVEf1G5_3DgCWGRS2t6JCzvDiLBm1\"\n",
        "ngrok.set_auth_token(token)\n",
        "\n",
        "# set up a stoppable thread (not mandatory, but cleaner if you want to stop this later\n",
        "class StoppableThread(threading.Thread):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(StoppableThread, self).__init__(*args, **kwargs)\n",
        "        self._stop_event = threading.Event()\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop_event.set()\n",
        "\n",
        "    def is_stopped(self):\n",
        "        return self._stop_event.is_set()\n",
        "\n",
        "def start_ngrok(q, stop_event):\n",
        "    try:\n",
        "        # Start an HTTP tunnel on the specified port\n",
        "        public_url = ngrok.connect(11434)\n",
        "        # Put the public URL in the queue\n",
        "        q.put(public_url)\n",
        "        # Keep the thread alive until stop event is set\n",
        "        while not stop_event.is_set():\n",
        "            time.sleep(1)  # Adjust sleep time as needed\n",
        "    except Exception as e:\n",
        "        print(f\"Error in start_ngrok: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8DKMKBt8psyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e2f0f4-6262-4659-dcde-c5269176202e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in start_ngrok: 'function' object has no attribute 'is_set'\n"
          ]
        }
      ],
      "source": [
        "# Create a queue to share data between threads\n",
        "url_queue = queue.Queue()\n",
        "# Start ngrok in a separate thread\n",
        "ngrok_thread = StoppableThread(target=start_ngrok, args=(url_queue, StoppableThread.is_stopped))\n",
        "ngrok_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WadAxHBpwr8",
        "outputId": "267565d5-22b9-4a25-f6c4-b79d85b3a975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok tunnel established at: NgrokTunnel: \"https://2d47-34-126-132-182.ngrok-free.app\" -> \"http://localhost:11434\"\n"
          ]
        }
      ],
      "source": [
        "# Wait for the ngrok tunnel to be established\n",
        "while True:\n",
        "    try:\n",
        "        public_url = url_queue.get()\n",
        "        if public_url:\n",
        "            break\n",
        "        print(\"Waiting for ngrok URL...\")\n",
        "        time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in retrieving ngrok URL: {e}\")\n",
        "\n",
        "print(\"Ngrok tunnel established at:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_gLBKxMGqr9V"
      },
      "outputs": [],
      "source": [
        "!export OLLAMA_HOST=https://2d47-34-126-132-182.ngrok-free.app/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bM8UtMJYqayN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
        "# Set environment variable for NVIDIA library\n",
        "# Set environment variables for CUDA\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZKV3He4qc0f",
        "outputId": "d7a9fbd7-ffcf-447b-ba81-81c428bcedb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ollama serve\n",
            "time=2024-05-02T06:42:40.474Z level=INFO source=images.go:817 msg=\"total blobs: 9\"\n",
            "time=2024-05-02T06:42:40.475Z level=INFO source=images.go:824 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-05-02T06:42:40.475Z level=INFO source=routes.go:1143 msg=\"Listening on 127.0.0.1:11434 (version 0.1.32)\"\n",
            "time=2024-05-02T06:42:40.476Z level=INFO source=payload.go:28 msg=\"extracting embedded files\" dir=/tmp/ollama461969675/runners\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n",
        "\n",
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PULLING MODELS**"
      ],
      "metadata": {
        "id": "N54LnCa4pPoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mistral"
      ],
      "metadata": {
        "id": "i0F-TJkspO9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFRAwg2iq3gp"
      },
      "outputs": [],
      "source": [
        "!ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_HGWxnCrA63"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfcVFlSh8XRK"
      },
      "source": [
        "# **CHAT INTERFACE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "LwvwTetX8dDD"
      },
      "outputs": [],
      "source": [
        "def process(url, question):\n",
        "  ##load data\n",
        "  # loader = PyPDFLoader(document)\n",
        "  loader = WebBaseLoader(url)\n",
        "  data = loader.load()\n",
        "  ## splitting\n",
        "  text_sp = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
        "  chunks = text_sp.split_documents(data)\n",
        "  ## embedding\n",
        "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True)\n",
        "  ## Vector store\n",
        "  vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"local-rag\"\n",
        "  )\n",
        "  ## Retreival\n",
        "  llm = ChatOllama(model=\"mistral\")\n",
        "  query_prompt = PromptTemplate(\n",
        "    input_variables= ['question'],\n",
        "    template= \"You are an AI language model assistant, your task is to generate a response for the question: {question}\"\n",
        "  )\n",
        "  retreiver = MultiQueryRetriever.from_llm(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    llm=llm,\n",
        "    prompt=query_prompt\n",
        "  )\n",
        "  # RAG Prompt\n",
        "  template = \"\"\"Answer the question based only on the following context:\n",
        "  {context}\n",
        "  Question: {question}\n",
        "  \"\"\"\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "  chain = (\n",
        "    {\"context\": retreiver, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "  )\n",
        "  return chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8pZlwBkqE3VA"
      },
      "outputs": [],
      "source": [
        "# Create a Gradio interface for file upload\n",
        "iface = gr.Interface(\n",
        "    fn=process,\n",
        "    inputs=[\"text\", \"text\"],\n",
        "    outputs=\"text\",\n",
        "    title=\"ChatDoc\",\n",
        "    description=\"Ask about a webpage.\"\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio interface\n",
        "iface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "-SIsMhS26SJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Streamlit app\n",
        "st.title(\"ChatDoc\")\n",
        "\n",
        "# Add a text input widget to get the user's input\n",
        "url = st.text_input(\"Enter the URL of the web page:\")\n",
        "question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "if st.button(\"Get answer\"):\n",
        "  # Call the function to process the user's input\n",
        "  answer = process(url, question)\n",
        "\n",
        "  # Display the answer to the user\n",
        "  st.write(\"Answer:\", answer)\n",
        "st.rerun()\n"
      ],
      "metadata": {
        "id": "QjwJVLHa7n9f"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how to run the code above as a streamlit app on colab\n",
        "\n",
        "!pip install streamlit\n",
        "!streamlit run app.py\n"
      ],
      "metadata": {
        "id": "WdvbsfxR8X_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXECUTING**"
      ],
      "metadata": {
        "id": "S7dtzPg0sKMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"intro.pdf\""
      ],
      "metadata": {
        "id": "QoEOvgpcsOfz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(file_path)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "JhG2K4GnslLN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is this about?\""
      ],
      "metadata": {
        "id": "M_930TdMstnh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process(file_path, question)"
      ],
      "metadata": {
        "id": "kjUmSvv0tIdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLM = Ollama(model='mistral')"
      ],
      "metadata": {
        "id": "qZEDfYg3tsno"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()\n",
        "chain = LLM | parser"
      ],
      "metadata": {
        "id": "i1bzLxA9yWuQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"what is the first country recognized the Independence of the United States?\")"
      ],
      "metadata": {
        "id": "moLX6fQTyLkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_context = \"My name is Mohamed, I was born at July, 24 2000, I am from Morocco.\"\n",
        "my_question = \"what is my nationality?\""
      ],
      "metadata": {
        "id": "fzMpp2cJzMsM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_template = \"Answer the question: {question} based on the context: {context}\"\n",
        "test_prompt = PromptTemplate.from_template(test_template)\n",
        "test_prompt.format(question= my_question, context= my_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XRVa146_1v9n",
        "outputId": "0de8ab82-4f22-4307-ca36-7b498e721559"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer the question: How old am I? based on the context: My name is Mohamed, I was born at July, 24 2000, I am from Morocco.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_chain = test_prompt | LLM | parser\n",
        "test_chain.invoke({\"question\": my_question, \"context\": my_context})"
      ],
      "metadata": {
        "id": "T0t2VIxQ2-zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "# Set logging to show only warnings and errors\n",
        "logging.basicConfig(level=logging.WARNING)\n"
      ],
      "metadata": {
        "id": "5XT6GBnvEd3b"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process(url=\"https://medium.com/riskified-technology/3-ways-to-break-into-data-science-6a7a8fd679b3\", question=\"Who is the author of this blog?\")"
      ],
      "metadata": {
        "id": "hxR88dnY3ig3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-34uvfDBylM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}